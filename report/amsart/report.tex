% =============================
% Fingerprinting LLMs Report
% Author: Dan McGonigle
% Advisor: Dr. Joris Roos
% =============================

\documentclass[12pt]{amsart}

% ---------- Packages ----------
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{booktabs,tabularx,siunitx,makecell}
\usepackage[final]{microtype} % better line breaks & glyph protrusion
\emergencystretch=3em         % gentle, global safety valve
\sisetup{table-number-alignment = center}
\setlength{\tabcolsep}{4pt} % tighter columns

% Bibliography with biblatex (adds Bibliography to TOC via bibintoc)
% If you prefer BibTeX, replace this block with natbib or plain thebibliography
\usepackage[backend=biber,style=ieee,sorting=nyt]{biblatex}
\addbibresource{references.bib}

% Clickable links appearance
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue,
  pdfauthor={Daniel McGonigle},
  pdftitle={Fingerprinting Large Language Models Using Signal Processing}
}

% Optional: line spacing for readability (uncomment if desired)
% \onehalfspacing

% ---------- Title Data ----------
\title{\textbf{Fingerprinting Large Language Models Using Signal Processing}}
\author{Daniel McGonigle \\[4pt] Advisor: Dr. Joris Roos}
\date{\today}

\begin{document}

% ---------- Title Page ----------
\begin{titlepage}
    \centering
    {\Huge \bfseries Fingerprinting Large Language Models Using Signal Processing\\[12pt]}
    \vspace{1.5cm}
    {\Large Daniel McGonigle\\[6pt]}

    {\large Advisor: Dr. Joris Roos}\\[12pt]

    {\large University of Massachusetts Lowell, Mathematics Department}\\[2pt]
    {\large Lowell, MA}\\[18pt]

    {\large \today}
    \vfill
\end{titlepage}

% Front matter uses roman numerals
\pagenumbering{roman}

% ---------- Abstract (≤150 words) ----------
\begin{abstract}
This project investigates whether text generated by large language models (LLMs) exhibits measurable statistical or spectral differences from human-generated text. Using token-level log-probability sequences for both human- and machine-generated text, we analyze distributional and signal-based features to identify potential "fingerprints" of model generation. Power-spectral densities, wavelet decompositions, and entropy measures were computed for human- and machine-authored corpora, revealing some measurable deviations in frequency dynamics between human and machine-generated sources. A classification model trained on these spectral representations achieved limited success in discriminating between human and model text, particularly when the same model used to generate the text provided the log-probability signal. These findings suggest that there may be merit in frequency-domain analysis as a means to detect text generated by advanced LLMs. 
\end{abstract}

% ---------- Acknowledgments ----------
\subsection*{Acknowledgments}
I would like to express my deepest gratitude to Dr. Joris Roos for his guidance, encouragement, and invaluable feedback throughout this project.  

% Ensure unnumbered sections appear in TOC
\addcontentsline{toc}{section}{Acknowledgments}

% ---------- Table of Contents ----------
\tableofcontents
\clearpage

% Switch to arabic page numbering for main text
\pagenumbering{arabic}


% ---------- Main Text ----------
% ---------- CHAPTER 1: Introduction ----------

\section{Introduction}
\label{ch:intro}
Since the release of GPT-3 ~\cite{brown2020gpt3}, large language models (LLMs) based on transformer architecture have revolutionized natural language processing by enabling fluent text generation that can closely mimic human style and reasoning. As the distinction between human- and machine-authored text becomes increasingly subtle, reliable methods for text that was produced by LLMs has become a significant challenge. Furthermore, there is value in attributing text to specific models, or identifying "fingerprints" imparted by particular models that aid in attribution. Applications range from academic integrity and misinformation tracking to model auditing and authenticity verification.

In this project, we explore an alternative perspective: that each model's generation process may leave a measurable spectral signature when its token probability sequence is treated as a signal. Specifically, we hypothesize that human and model text differ in the temporal and frequency-domain characteristics of these probability signals due to differences in attention dynamics and sampling noise.

To investigate this hypothesis, we conducted a systematic comparison between human-written and LLM-generated text using Fourier analysis and wavelet transforms applied to token log-probability sequences. The analysis focused on identifying characteristic frequency bands, entropy levels, and power-spectrum shapes that could distinguish machine- from human-generated text. Complementary statistical and distributional metrics aimed at probing frequency characteristics were also used to evaluate the separability of these groups in a non-spectral space.

There are two goals in this work: The first goal is to assess whether frequency-domain analysis provides meaningful discriminatory power between human and model text. The second is to establish a foundation for spectral fingerprinting, with the hope that this can be done in a model-agnostic manner. The findings of this study demonstrate some promising signal-level regularities that suggest LLMs possess spectral patterns across generations, motivating future research into cross-model generalization, temporal dynamics of attention mechanisms, and the integration of spectral features into broader model-audit frameworks.


% ---------- CHAPTER 2: Background and Related Work ----------
\section{Background and Related Work}
\label{ch:background}

This body of works covers a lot of ground from LLMs and detecting generative content, to tools for analyzing signals in the frequency domain, including computer science methodologies for classification.  This chapter serves as a brief introduction to these topics and discusses some of the relevant work.

% ---------- CHAPTER 2: Background and Related Work ----------
% ---------- SECTION 1: LLMs ----------
\subsection{Large Language Models}
\label{sec:llms}

LLMs are autoregressive neural networks trained to predict the next token in a sequence given all preceding context. During generation, each output token is sampled from a probability distribution $P(t_i \mid t_{<i})$, representing the model's estimated likelihood of possible continuations at position $i$. These token-level probabilities capture a model's evolving internal state and confidence: when the model is highly certain, the distribution is sharply peaked; when uncertain, it is flatter. The logarithm of these probabilities, or \emph{log-probabilities}, are particularly useful because they linearize multiplicative relationships, stabilize numerical variation, and directly reflect the additive structure of sequence likelihoods.

% ---------- CHAPTER 2: Background and Related Work ----------
% ---------- SECTION 2: Detecting Machine-Generated Text ----------
\subsection{Detecting Machine-Generated Text}
\label{sec:llm_detection}

Most existing detection techniques focus on lexical or syntactic cues, statistical irregularities such as word, phrase or punctuation probabilities, or leveraging neural networks as in ~\cite{wu2023survey}. Some approaches targeting specific data domains have showed limited success, as in DetectRL~\cite{wu2024detectrl}. Zhang et al.~\cite{zhang2024zeroshot} introduced a zero-shot detection approach that operates directly on token probability distributions, showing that simple statistical measures such as likelihood variance and divergence between human and model token-prob histograms can achieve strong performance (AUROC $\approx 0.9$) when distinguishing GPT-3 and ChatGPT text from human writing. Yet these methods often fail to generalize across models or fine-tuning conditions. 

% ---------- CHAPTER 2: Background and Related Work ----------
% ---------- SECTION 3: Analyzing Signals in the Frequency Domain ----------
\subsection{Analyzing Signals in the Frequency Domain}
\label{sec:freq_analysis}

Signal analysis in the frequency domain provides a powerful framework for understanding the underlying structure, periodicity, and energy distribution of time-varying data. Rather than analyzing a signal \( x(t) \) in the time domain—where the focus is on its instantaneous amplitude or value at each time point—frequency domain analysis decomposes the signal into constituent sinusoids of different frequencies, allowing researchers to study the spectral content and dominant oscillatory components.

\subsubsection{The Fourier Transform and Its Discrete Counterparts}

The \textit{Fourier Transform (FT)} expresses a signal as a sum of complex exponentials, mapping it from the time domain to the frequency domain~\cite{oppenheim_signals_systems_ocw}. For a continuous signal \( x(t) \), the Fourier Transform is defined as

\begin{equation}
    X(f) = \int_{-\infty}^{\infty} x(t)\, e^{-j2\pi f t}\, dt,
\end{equation}

where \( X(f) \) represents the complex-valued frequency spectrum. In analyzing the token-level log-probabilities of LLM output, the "time" domain for this research consists of discrete token positions. This use of discrete samples requires us to utilize the \textit{Discrete Fourier Transform (DFT)}, which is defined:

\begin{equation}
    X_k = \sum_{n=0}^{N-1} x_n\, e^{-j 2\pi k n / N}, 
    \quad k = 0, 1, \ldots, N-1.
\end{equation}

The DFT converts a sequence of \( N \) samples into \( N \) complex coefficients corresponding to frequency bins. Efficient computation of the DFT is achieved using the \textit{Fast Fourier Transform (FFT)} algorithm~\cite{cooley1965algorithm}, which reduces computational complexity from \( O(N^2) \) to \( O(N \log N) \). The FFT underpins much of spectral analysis, providing useful tools for power spectral density estimation, filtering, and feature extraction in both scientific and engineering applications.

\subsubsection{Limitations of Purely Frequency-Domain Analysis}

While the Fourier Transform is effective for stationary signals—those whose frequency composition does not change over time—it is less suited for \textit{nonstationary} or \textit{transient} data, where the frequency content varies. The Fourier spectrum captures signal patterns by determining global frequency coefficients, but loses features that are temporally localized. This limitation motivated the development of \textit{time-frequency representations}, which aim to capture how frequency content evolves over time.

\subsubsection{Wavelet Transforms and Time-Frequency Analysis}

The \textit{Wavelet Transform (WT)} addresses the shortcomings of the Fourier framework by providing a multi-resolution view of a signal. Instead of fixed sinusoidal bases, wavelet analysis uses localized ``wavelets''—functions that are dilated and translated versions of a mother wavelet—to analyze both short, high-frequency events and long, low-frequency trends~\cite{mallat1989theory,oppenheim_signals_systems_ocw}.  

For a continuous-time signal \( x(t) \), the \textit{Continuous Wavelet Transform (CWT)} is defined as

\begin{equation}
    W(a, b) = \frac{1}{\sqrt{|a|}} 
    \int_{-\infty}^{\infty} x(t)\, 
    \psi^*\!\left(\frac{t - b}{a}\right)\, dt,
\end{equation}

where \( a \) and \( b \) denote the scale and translation parameters, respectively, and \( \psi(t) \) is the mother wavelet. The \textit{Discrete Wavelet Transform (DWT)} discretizes these parameters, yielding efficient computational schemes with compact representations of the signal across scales.

Wavelet analysis has proven useful for analyzing signals with nonstationary or multi-scale structure—ranging from geophysical and biomedical signals to linguistic data~\cite{addison2017illustrated}.


% ---------- CHAPTER 3: Methods ----------
\section{Methods}
\label{ch:methods}

In this research, these per-token log-probabilities are treated not merely as statistical outcomes but as a \emph{temporal signal} that evolves as the model generates text. Each step in generation corresponds to a new sample in a discrete-time series whose fluctuations encode the model's shifting certainty, stylistic rhythm, and contextual transitions. This framing allows the use of signal-processing tools---such as Fourier and wavelet transforms---to examine structure in the \emph{frequency domain} rather than only the token-distribution domain. If model outputs differ systematically from human writing in the smoothness, periodicity, or spectral composition of their log-probability sequences, these differences can be interpreted as latent \emph{fingerprints} of the model's internal generative dynamics. By analyzing log-probabilities as signals, we aim to uncover whether LLMs exhibit distinctive frequency-domain patterns that remain stable across text samples and model families, providing a potential foundation for model attribution and authenticity detection.

% ---------- CHAPTER 3: Methods ----------
% ---------- SECTION 1: Dataset Curation ----------
\subsection{Dataset Curation}
\label{sec:dataset_curation}
A rigorous and transparent dataset curation process is essential for any research endeavor involving linguistic or generative text analysis. This process encompasses corpus selection, data acquisition, cleaning, annotation, and metadata management. For the present study, the goal was to construct a corpus that balances diversity of authorship, disciplinary representation, and writing quality, while maintaining consistency in format and linguistic features suitable for quantitative and spectral analysis. We sought several key characteristics to amplify signal differences related to LLM patterns and minimize signal differences based on domain-specific text, formatting errors or technical jargon:

\begin{enumerate}[label=\arabic*., leftmargin=2em]
    \item To prevent "leakage" of LLM-generated text into the human-generated corpus, we select datasets that were curated before 2020.
    \item We avoid signal differences based on poor writing style by selecting documents that are professionally or semi-professionally written.
    \item Care is taken to avoid patterns that are irrelevant, such as structured elements (bibliography, table of contents, etc).
\end{enumerate}

\subsubsection{Source Corpus: Reuters~50/50 (Subset of RCV1)}
\label{sec:reuters5050}

The primary dataset used in this study is the \textit{Reuters~50/50} corpus, a balanced subset derived from the \textit{Reuters Corpus Volume~1 (RCV1)}~\cite{lewis2004rcv1}.  
The full RCV1 dataset contains over 800{,}000 newswire stories published by Reuters Ltd.\ between August~20,~1996 and August~19,~1997.  
Each article was manually and algorithmically categorized into hierarchical topic codes, industry sectors, and regions, forming one of the most influential benchmark corpora for text classification, information retrieval, and distributional semantics.

\paragraph{Dataset composition.}
The Reuters~50/50 subset is a widely used balanced sampling of the RCV1 corpus designed for binary classification and stylistic comparison tasks.  
It consists of 50 selected topic categories drawn from the full taxonomy, with approximately equal representation of documents per category, resulting in a roughly uniform distribution across thematic domains such as politics, economics, international relations, science, and culture.  
This balance reduces topical bias and supports fair evaluation of linguistic or statistical features independent of subject matter.

\paragraph{Preprocessing and normalization.}
All Reuters documents were processed in a way to minimize patterns that might manifest as signal differences between human- and machine-generated text. Documents were normalized to UTF--8, spelling was corrected, and dates were standardized to a consistent format. 

\paragraph{Generation.}
The LLM-generated corpus was produced using Meta's \textit{Llama~3.1} model, a 70\,billion-parameter transformer architecture deployed in quantized form for efficient inference. Through iterative prompt engineering, a standardized procedure was developed to generate machine-authored documents that mirror the stylistic and structural characteristics of human writing while avoiding confounding artifacts such as domain-specific jargon, formatting inconsistencies, or explicit references to named entities. The objective was to construct a corpus in which any measurable signal differences between human- and model-generated text could be attributed primarily to stylistic and probabilistic properties of the language model itself rather than to topical or contextual factors.

\medskip
\noindent
The generation methodology for each human-written document proceeded as follows:

\begin{enumerate}[label=\arabic*., leftmargin=2em]
    \item The LLM was prompted to generate a document within the same general topical domain as the source text (e.g., business, finance, politics).
    \item The model was instructed to approximate the \textit{length}, \textit{style}, and \textit{organizational structure} of the corresponding human-written sample, including paragraph count and rhetorical tone.
    \item Where applicable, the LLM was encouraged to reuse proper names, recurring entities, or unique subjects from the original to maintain thematic alignment while still producing novel phrasing.
\end{enumerate}

\noindent
This approach yielded a synthetic corpus that is topically aligned and stylistically comparable to the human-authored datasets (MICUSP and Reuters~50/50), enabling controlled comparative analysis of statistical and spectral features.

\subsubsection{Source Corpus: The Michigan Corpus of Upper-Level Student Papers (MICUSP)}

A secondary dataset used in preliminary analysis for this study is the \textit{Michigan Corpus of Upper-level Student Papers (MICUSP)}, a well-documented collection of academic writing produced by proficient undergraduate and graduate students at the University of Michigan~\cite{roemer2011micusp,odonnell2012micusp}.  
The corpus was developed to represent high-quality student writing across a wide range of academic disciplines, capturing authentic examples of advanced learner language in academic contexts.  

Römer and O'Donnell~\cite{roemer2011micusp} describe the design and compilation of MICUSP (Part 1), emphasizing the corpus's multi-disciplinary structure and genre taxonomy. The resulting collection comprises approximately 830 student papers totaling 2.6 million words, with metadata including discipline, genre, grade level, native language, and gender of the author.

% ---------- CHAPTER 3: Methods ----------
% ---------- SECTION 2: Token-Level Grading of Text Samples ----------
\subsection{Token-Level Grading of Text Samples}
\label{sec:grading}

To enable a consistent comparison between human- and model-generated writing, we represent each text as a sequence of token-level probabilities as assigned by a reference large language model (LLM). We use the term \emph{grading} to refer to the generation of token-level log probabilities for a given text. We accomplish this by passing a text sample through an LLM in a non-generative (evaluation-only) mode and recording the model's conditional log-probability for each observed token, given all preceding context. Formally, for a text sequence $x = (x_1, x_2, \dots, x_T)$, we obtain
\[
    \ell_t = \log P_\theta(x_t \mid x_{<t}),
\]
where $P_\theta$ denotes the probability distribution defined by the model with parameters $\theta$. The resulting series $\{\ell_t\}_{t=1}^T$ represents the model's internal assessment of how \emph{expected} or \emph{surprising} each token is within its surrounding context.

\subsubsection*{Motivation and Terminology}
We adopt the term \emph{grading} to describe this process because it parallels how a human evaluator might assign a score to a written text, albeit at a much finer (token-level) granularity. The grading model effectively produces a per-token "confidence profile" that reflects its own linguistic expectations and calibration. When applied to human-written text, these scores reflect how well the human sequence aligns with the model's learned distribution of language; when applied to model-generated text, they measure the model's self-assessment or, in cross-model cases, one model's evaluation of another's output.

\subsubsection*{Procedure}
For each document in our dataset, we performed the following steps:
\begin{enumerate}
    \item Tokenize the text using the same tokenizer as the evaluating model (e.g., LLaMA or Mixtral tokenizer).
    \item Feed the full token sequence to the model in evaluation mode (\texttt{generate=False}) to obtain conditional log-probabilities for all tokens.
    \item Record and store the sequence of token log-probabilities, normalized probabilities, and optional derived features such as per-token entropy and perplexity.
\end{enumerate}
This grading process was performed for both human-authored and LLM-generated documents, enabling direct statistical comparison between groups. Importantly, the same evaluation model was used consistently within each experimental condition to ensure comparability of scores.

\subsubsection*{Experimental Groupings}
Using this method, we generated three distinct graded datasets:
\begin{enumerate}
    \item \textbf{Human-generated, LLaMA-graded (HG--LG):} Human-written documents graded by LLaMA.
    \item \textbf{LLaMA-generated, LLaMA-graded (LG--LG):} Model-generated documents graded by LLaMA.
    \item \textbf{LLaMA-generated, Mixtral-graded (LG--MG):} Model-generated documents graded by Mixtral, to study cross-model calibration and evaluator bias.
\end{enumerate}
Each group thus consists of the same fundamental signal type—a sequence of token-level log-probabilities—but derived under different combinations of generator and evaluator. Throughout the remainder of this report, we refer to these sequences as \emph{graded signals}, and their statistical and spectral properties form the basis of the analyses that follow.

% ---------- CHAPTER 3: Methods ----------
% ---------- SECTION 3: Analysis of Distributions ----------
\subsection{Analysis of Distributions}
\label{sec:dist_analysis}

A central question in this study is whether statistical properties of token-level signals differ systematically between human- and model-generated text, and whether those differences remain detectable when using models other than the text-generating model. The preliminary analysis was to first examine the marginal and joint distributions of token log-probabilities and related features. These analyses serve two primary purposes: (1) to characterize surface-level statistical differences that may underlie or interact with deeper temporal patterns, and (2) to assess how such differences vary depending on whether the text was generated or graded by different large language models (LLMs).

Specifically, we compare three experimental groupings:
\begin{enumerate}
    \item \textbf{Human-generated, LLaMA-graded (HG--LG):} Human-written documents graded by the LLaMA model.
    \item \textbf{LLaMA-generated, LLaMA-graded (LG--LG):} Model-generated documents graded by the same model that produced them.
    \item \textbf{LLaMA-generated, Mixtral-graded (LG--MG):} Model-generated documents graded by a distinct LLM (Mixtral) to test grading-model dependence.
\end{enumerate}

For each group, we extract per-token log-probabilities, normalized probabilities, and derived quantities such as entropy. Distributions are then analyzed both marginally and through pairwise comparisons. Empirical cumulative distribution functions (ECDFs), kernel density estimates (KDEs), and Zipf plots are used to visualize deviations between groups. Statistical divergence metrics—including Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and the Wasserstein distance—quantify how strongly each pair of distributions differs.

The goal of this analysis is not merely to detect separability, but to characterize *what form* these separations take. For example, a consistent rightward shift in the LLaMA-generated distributions would suggest that the model assigns higher confidence to its own outputs compared to human-written text, whereas discrepancies between LLaMA-graded and Mixtral-graded distributions may indicate calibration or alignment differences between evaluators. Together, these distributional experiments provide a statistical foundation for subsequent analyses in the frequency domain, where we explore how such differences manifest as structured temporal or spectral patterns across token sequences.

% ---------- CHAPTER 3: Methods ----------
% ---------- SECTION 4: Analysis of Signals ----------
\subsection{Analysis of Signals}
\label{sec:signal_analysis}

While distributional comparisons reveal static statistical differences between groups, they do not capture how those differences evolve or manifest over the sequence of tokens. To investigate temporal structure, we treat each graded text as a discrete one-dimensional signal composed of token-level log-probabilities, $\{\ell_t\}_{t=1}^T$. We then analyze these sequences using tools from signal processing, with the aim of identifying rhythmic or self-similar patterns characteristic of human versus model generation.

The methods described in this section include frequency-domain analysis using the Fourier transform, time-frequency analysis using wavelet transforms, and additional time-series techniques such as variance, autocorrelation, and entropy-based metrics.

\subsubsection{Fourier Transform Analysis}
\label{subsec:fourier_methods}

Having introduced the theoretical basis of the Fourier transform earlier, here we focus on the specific spectral features derived from the token-level log-probability signals. Each sequence $\{\ell_t\}$ was treated as a discrete signal and transformed using the real-valued fast Fourier transform (FFT) to obtain its power spectrum, $P_k = |\hat{\ell}_k|^2$. The power spectrum captures how the total signal variance is distributed across frequency components, reflecting the degree of smoothness or volatility in the underlying confidence dynamics.

From each normalized power spectrum, we computed the following summary features:

\begin{itemize}
    \item \textbf{High-frequency energy ratios} (\verb|fft_high_energy_ratio_cut{c}|), with $c \in \{0.5, 0.75, 0.9\}$:

    The fraction of total spectral power in frequencies $\ge c$ times the Nyquist frequency. Formally,
    \[
        R_c = \frac{\sum_{k \geq \lfloor c \cdot K \rfloor} P_k}{\sum_{k=0}^{K-1} P_k},
    \]
    where $K$ is the number of FFT bins. Larger ratios indicate greater concentration of energy at higher frequencies, corresponding to more rapid, localized fluctuations in model-assigned confidence. Lower ratios reflect smoother, more predictable token-probability sequences.

    \item \textbf{Spectral centroid:}  
    The power-weighted mean frequency, defined as
    \[
        f_{\text{centroid}} = \frac{\sum_k k \, P_k}{\sum_k P_k}.
    \]
    This metric indicates where the “center of mass” of spectral energy lies. Human-written text typically exhibits lower centroids (energy concentrated at lower frequencies), whereas model-generated text may have higher centroids reflecting finer-scale oscillations in confidence.

    \item \textbf{Spectral slope:}  
    The slope of a least-squares linear fit of $\log(P_k + \varepsilon)$ versus frequency index $k$, where $\varepsilon$ prevents numerical instability for near-zero powers. This measure captures the overall decay rate of spectral energy with frequency. Steeper negative slopes indicate dominance of low frequencies (smooth, structured confidence evolution), while flatter or positive slopes suggest relatively more high-frequency noise or abrupt variability.
\end{itemize}

Together, these spectral metrics provide compact, interpretable summaries of signal smoothness and temporal coherence. They complement the distributional and autocorrelation analyses by quantifying the relative balance of slow versus fast variations in token-level confidence, thereby revealing characteristic frequency signatures that may differentiate human from model text.

\subsubsection{Wavelet Transform Analysis}
\label{subsec:wavelet_methods}
While Fourier transforms reveal global frequency content, they are less suited to identifying how those frequencies vary locally in time. To capture both time and frequency information simultaneously, we apply continuous and discrete wavelet transforms (CWT and DWT). The wavelet transform of $\ell_t$ with respect to a mother wavelet $\psi$ is defined as
\[
    W(a, b) = \frac{1}{\sqrt{a}} \sum_t \ell_t \, \psi^*\!\left( \frac{t - b}{a} \right),
\]
where $a$ and $b$ represent scale and translation parameters, respectively.

We explored several wavelet families with different time-frequency trade-offs:
\begin{itemize}
    \item \textbf{Daubechies (db2, db4, db8):} Provide compact support and varying smoothness. Lower-order Daubechies capture sharper transitions, while higher-order ones capture smoother, more global patterns in the signal.
    \item \textbf{Symlets (sym4):} Nearly symmetrical variants of Daubechies wavelets that reduce phase distortion, allowing more interpretable localization of transient features.
    \item \textbf{Coiflets (coif1):} Offer improved symmetry and higher vanishing moments for both the wavelet and scaling functions, making them well-suited for capturing slow-varying confidence trends.
    \item \textbf{Biorthogonal (bior3.3):} Enable separate decomposition and reconstruction filters, allowing perfect signal reconstruction with increased symmetry and linear phase—useful for comparing wavelet energies across documents of different lengths.
\end{itemize}
Wavelet coefficients were used to compute multi-scale energy spectra and entropy measures, which quantify how signal complexity varies across scales. By averaging over documents within each group, we obtained characteristic signatures describing the temporal organization of human- versus model-generated token probabilities.

\subsubsection{Statistical Signal Features}
\label{subsec:statistical_methods}
In addition to spectral and wavelet analyses, we examined several time-series properties of the graded signals to capture different aspects of variability and temporal dependence:
\begin{itemize}
    \item \textbf{Coefficient of Variation:} Amplitude fluctuations in log-probability over a rolling window; lower variance suggests smoother, more confident generation.
    \item \textbf{Autocorrelation:} Measures how strongly a token's log-probability depends on preceding tokens. We compute the autocorrelation function $r(\tau)$ for lags $\tau = 1, 2, \dots$, as well as the integrated autocorrelation time to summarize persistence of confidence dynamics.
    \item \textbf{Token-Token Difference Variance:} We compute the variance of the difference between each adjacent pair of tokens, which captures the local smoothness of the probability signal.
\end{itemize}
These methods complement the spectral analyses by providing interpretable, scale-agnostic features describing signal stability, smoothness, and self-similarity.

% ---------- CHAPTER 3: Methods ----------
% ---------- SECTION 5: Classification and Validation ----------
\subsection{Classification and Validation}
\label{sec:classification}

Having extracted a diverse set of statistical and spectral features from the graded signals, we next evaluated their ability to discriminate between human- and model-generated text. This section describes the procedures used to (1) identify effective thresholds for individual features, (2) combine multiple features using a random forest classifier, and (3) validate classification performance through both cross-validation and independent train-test splits.

\subsubsection{Feature-Level Threshold Analysis}
For each scalar feature (e.g., spectral centroid, high-frequency energy ratio, autocorrelation time, entropy), we examined its separability between groups by fitting a simple one-dimensional threshold classifier. Thresholds were determined by maximizing accuracy on the training set or, equivalently, by selecting the cut-point that minimized total classification error across the two classes.  
Receiver Operating Characteristic (ROC) curves and Area Under the ROC Curve (AUROC) values were computed to quantify discriminative strength for each feature independently. The F1 harmonic score was used as a threshold-independent metric for accuracy, factoring in precision and recall. This step provided interpretable baselines and helped identify which features carried the most individual predictive signal prior to multivariate modeling.

\subsubsection{Random Forest Classification}
To jointly model multiple, potentially interacting features, we employed a random forest classifier (RFC) implemented with \texttt{scikit-learn}. Random forests are ensemble models composed of multiple decision trees trained on bootstrapped subsets of the data, each considering a random subset of available features at each split. The final prediction is obtained by majority vote over all trees.  
Feature vectors were standardized prior to training, and hyperparameters such as the number of estimators, tree depth, and minimum samples per leaf were tuned through grid search to balance bias and variance. The model outputs both class predictions and feature importance scores, allowing interpretation of which features most strongly contributed to discrimination.

\subsubsection{Cross-Validation and Train/Test Evaluation}
Model performance was assessed using both $k$-fold cross-validation and independent train/test splits to ensure robustness.  
\begin{itemize}
    \item \textbf{Cross-validation:} We used stratified $k$-fold cross-validation (typically $k=5$) to estimate variability in model performance across folds. For each fold, metrics including accuracy, precision, recall, F$_1$, and AUROC were computed and averaged.
    \item \textbf{Train/test splits:} In addition to cross-validation, a held-out test set (commonly 20\% of the data) was used for final model evaluation. The model was trained on the remaining 80\% and tested once on unseen samples to approximate out-of-sample generalization performance.
\end{itemize}
This dual evaluation strategy ensured that both threshold-based and multivariate classifiers were validated against overfitting and provided reliable estimates of discriminative performance.

\subsubsection{Interpretation}
Threshold analysis allowed us to identify features with strong individual separation power, while the random forest classifier leveraged complementary information across heterogeneous features. Together, these approaches quantify the extent to which statistical and spectral properties of the graded signals encode detectable differences between human- and model-generated text, forming the methodological foundation for the performance results presented in the following chapter.


% ---------- CHAPTER 4: Results ----------
\section{Results}
\label{ch:results}

% ---------- CHAPTER 4: Methods ----------
% ---------- SECTION 1: Statistical Analysis ----------
\subsection{Statistical Analysis}
\label{sec:statistical_analysis}
The ECDF and aggregate token log-probability distribution graphics indicate that there are some clear statistical differences between human-authored and AI-generated text. Further

Distribution distance measures indicate that the difference between log probabilities from human- and machine-generated text is an order of magnitude greater than the difference between machine-generated text that is "graded" by the same model vs. a different family of model:

\sisetup{table-number-alignment=center, table-figures-integer=1, table-figures-decimal=6}

\begin{table}[h!]
\centering
\small
\caption{Divergence and Distance Metrics Between Aggregated Distributions}
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}X
  S[table-format=1.6]
  S[table-format=1.6]
  S[table-format=1.6]
  S[table-format=1.6]
  S[table-format=1.6]
  S[table-format=1.6]}
\toprule
\textbf{Pair} &
\multicolumn{1}{c}{\textbf{KL$(a\mid\mid b)$}} &
\multicolumn{1}{c}{\textbf{KL$(b\mid\mid a)$}} &
\multicolumn{1}{c}{\textbf{JSD}} &
\multicolumn{1}{c}{\textbf{$\sqrt{\text{JSD}}$}} &
\multicolumn{1}{c}{\textbf{KS}} &
\multicolumn{1}{c}{\makecell{\textbf{Wasser-}\\\textbf{stein-1}}} \\
\midrule
Human (agg) vs LLaMA (agg)   & 0.156811 & 0.122653 & 0.032095 & 0.179150 & 0.204269 & 0.873242 \\
Human (agg) vs Mixtral (agg) & 0.144633 & 0.122374 & 0.029919 & 0.172971 & 0.168239 & 0.787727 \\
LLaMA (agg) vs Mixtral (agg) & 0.013986 & 0.023439 & 0.002481 & 0.049805 & 0.050220 & 0.113508 \\
\bottomrule
\end{tabularx}
\end{table}

Finally, one of the most compelling pieces of visual evidence from analyzing the log-probability signal characteristics using a moving average chart. You can see that the high frequency patterns for an individual document are far more greatly pronounced in human-generated text than machine-generated text. There is additionally a small difference in the high frequency patterns between the LLM-generated text as graded by the same model (Llama) vs. another model (Mixtral). This is consistent with the aggregate token log-probability distribution chart.

% ---------- CHAPTER 4: Methods ----------
% ---------- SECTION 2: Signal Analysis ----------
% \subsection{Signal Analysis}
% \label{sec:signal_analysis}


% ---------- CHAPTER 5: Conclusion and Future Works ----------
\section{Conclusion and Future Work}
\label{ch:conclusion}
In this study, we compared aggregated distributions of human- and model-generated text using a range of divergence and distance metrics. The results highlight measurable but nuanced differences between the underlying probability structures of the evaluated sources, suggesting that statistical signatures of large language models can be detected even when aggregated across samples. 

Future work will expand on these findings by incorporating additional models, finer-grained feature representations, and temporal analyses of model drift. We also plan to investigate the robustness of these divergence-based metrics under varying sampling conditions and explore their integration into a broader detection framework combining statistical and deep learning approaches.


% ---------- References ----------
\clearpage
\phantomsection
\printbibliography[heading=bibintoc,title={References}]

% ---------- Appendices ----------
\appendix

\section{Additional Tables and Figures}
\label{ch:addl_figures}


\section{Implementation Details}
\label{ch:implementation_details}


\end{document}

